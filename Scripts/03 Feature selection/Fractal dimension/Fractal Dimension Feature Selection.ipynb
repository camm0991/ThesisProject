{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries, utilities and definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from math import log\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from scipy.stats import linregress\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from time import time\n",
    "from timeit import timeit\n",
    "\n",
    "#Returns 1 if a point is inside a radius, if not, returns 0\n",
    "def dist(p, r):\n",
    "    return 1 if p <= r else 0\n",
    "\n",
    "#Makes the distance function aplicable to numpy arrays\n",
    "check_dist = np.vectorize(dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fractal dimension feature selection algorithm\n",
    "The algorithm is adjusted to the dataset of the experiment so the number of attributes must be modified, it calculates the approximated fractal dimension after deleting an attribute, if the new value is inside the threshold value it can be eliminated and ends when no attribute is delete from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def fractal_feature_selection(df, threshold=0.09):\n",
    "    \n",
    "    #Obtains the approximate fractal dimension of the original dataset\n",
    "    base_fd = fractal_dimension(df)\n",
    "    print('Whole dataset approximate fractal dimension: {}'.format(base_fd))\n",
    "    \n",
    "    #List for keeping track of the attributes index in ther starting order\n",
    "    sorted_attrib = [[0, 0], [1, 1], [2, 2], [3, 3], [4, 4], [5, 5], \n",
    "                     [6, 6], [7, 7], [8, 8], [9, 9], [10, 10], [11, 11], [12, 12], [13, 13]]\n",
    "    \n",
    "    attribute_not_deleted = True\n",
    "    while attribute_not_deleted:\n",
    "        fd_list = []\n",
    "        for i in sorted_attrib:\n",
    "            #Deletes i attribute from dataset\n",
    "            X = np.delete(df, i[0], axis=1)\n",
    "            partial_fd = fractal_dimension(X)\n",
    "            #Adds the information of the approximate fractal dimension to a list to obtain the one that \n",
    "            #contribute less to the whole dataset\n",
    "            fd_list.append([i[0], \n",
    "                            partial_fd, \n",
    "                            abs((partial_fd / indicator_fd) - 1), \n",
    "                            abs((partial_fd / indicator_fd) - 1) < threshold])\n",
    "            \n",
    "        #Sort by partial fractal dimension value\n",
    "        fd_list.sort(key = lambda row: row[2])\n",
    "        \n",
    "        for i in fd_list:\n",
    "            #Checks if the variation of the fractal dimension value is inside the threshold\n",
    "            if i[3] == True:\n",
    "                #Update fractal dimension value\n",
    "                indicator_fd = i[1]\n",
    "                #Deletes attribute that doesn't contributes more the threshold value to the farctal dimension value\n",
    "                df = np.delete(df, i[0], axis=1)\n",
    "                #Deletes the i attribute from our reference list\n",
    "                sorted_attrib = np.delete(sorted_attrib, i[0], axis=0)\n",
    "                #Decremets the relative value of the attributes to the right of the deleted one\n",
    "                for j in xrange(i[0], len(sorted_attrib)):\n",
    "                    sorted_attrib[j][0] -= 1\n",
    "                break\n",
    "            #No attribute was deleted\n",
    "            attribute_not_deleted = False\n",
    "    return sorted_attrib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fractal dimesion of a dataset\n",
    "This algorithm calculates the approximate fractal dimension of the given dataset which must be loaded on a numpy data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def fractal_dimension(dataset):\n",
    "    #Data set cardinality\n",
    "    N = len(dataset)\n",
    "    #Results list of correlation integral values\n",
    "    cm = []\n",
    "    #List of radius to test distance between points\n",
    "    r = [1.0];\n",
    "    r_index = 0;\n",
    "    \n",
    "    #Executes while the sumation is greater than 0\n",
    "    tempSumation = 0\n",
    "    while True:\n",
    "        #Number of points that return 1 in the heaviside function\n",
    "        sumation = 0\n",
    "        #Obtaining distance between point Xi and all of the others\n",
    "        for j in range(N-1):\n",
    "            euclidean_dist_array = euclidean_distances(dataset[j].reshape(1, -1), dataset[j+1:])\n",
    "            sumation += np.sum(check_dist(euclidean_dist_array, r[r_index]))\n",
    "        if sumation <= 0 or tempSumation == sumation:\n",
    "            break;\n",
    "        cm.append((2.0 * sumation) / (N * (N - 1.0)))\n",
    "        r.append(r[r_index] / 2.0)\n",
    "        tempSumation = sumation\n",
    "        r_index += 1       \n",
    "    \n",
    "    #Deletes extra value in r\n",
    "    del r[-1]\n",
    "        \n",
    "    #Calculate ln of both r and cm\n",
    "    ln_r = map(log,r)\n",
    "    ln_cm = map(log,cm)\n",
    "    \n",
    "    #Calculate linear regresion of the points\n",
    "    slope_as_fd, _, _, _, _ = linregress(ln_r,ln_cm)\n",
    "\n",
    "    #Return slope as aproximate fractal dimension\n",
    "    return slope_as_fd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fetching files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['EEG_Train_Filtered_0.5_30Hz_l4.csv', 'EEG_Train_Sorted.csv']\n"
     ]
    }
   ],
   "source": [
    "start_time = time()\n",
    "path = \"..\\..\\Data\\The Tesis EEG\\Train\"\n",
    "files = [f for f in listdir(path) if isfile(join(path, f))]\n",
    "print(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing the dataset\n",
    "In this experiment the training data is analyzed using a threshold value of 0.005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EEG_Train_Filtered_0.5_30Hz_l4.csv\n",
      "Whole dataset approximate fractal dimension: 7.14545607293\n",
      "EEG_Train_Sorted.csv\n",
      "Whole dataset approximate fractal dimension: 5.24572496911\n",
      "Threshold = 0.005\n",
      "0 1 1 1 1 0 1 1 1 1 0 1 1 1 \n",
      "0 1 1 0 1 0 1 1 1 1 1 1 1 1 \n",
      "\n",
      "Elapsed time: 36182.3209999\n"
     ]
    }
   ],
   "source": [
    "threshold_values = [0.005]\n",
    "#Apply fractal dimension feature selection to all the datasets in the folder for each one of the threshold values\n",
    "for i in threshold_values:\n",
    "    results = []\n",
    "    for j in files:\n",
    "        print(j)\n",
    "        stdsc = StandardScaler()\n",
    "        df = pd.read_csv(path + '\\\\' + j)\n",
    "        X = df.ix[:,0:14]\n",
    "        X_std = stdsc.fit_transform(X)\n",
    "        X_std = np.array(X_std)\n",
    "    \n",
    "        results.append(fractal_feature_selection(X_std, i))\n",
    "\n",
    "    #Interpretation oh the obtained results\n",
    "    print('Threshold = {}'.format(i))\n",
    "    for k in results:\n",
    "        ref = [0,1,2,3,4,5,6,7,8,9,10,11,12,13]\n",
    "        for l in k:\n",
    "            ref[l[1]] = -1\n",
    "        for l in ref:\n",
    "            if l >= 0:\n",
    "                print('0'),\n",
    "            else:\n",
    "                print('1'),\n",
    "        print('')\n",
    "print('\\nElapsed time: {}'.format(time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see that on both datasets (filtered and not filtered) three attributes are deleted, the first and the sixth are eliminated on both of them which is interesting."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
